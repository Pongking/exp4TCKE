{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af8d9a1b-27f5-4861-a28b-4f03da61f543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "#加载bert模型的分词器\n",
    "from transformers import BertModel\n",
    "#用于加载bert模型\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daa83afc-9234-4e82-b108-bc2c30f96041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "#每批次训练的数据量\n",
    "batch_size=16\n",
    "text_max_length=128\n",
    "#训练100轮\n",
    "epochs=100\n",
    "lr=3e-5\n",
    "validation_ratio=0.1\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#50次打印一次\n",
    "log_per_step=50\n",
    "#定义path\n",
    "datasets_dir=Path(\"./datasets\")\n",
    "os.makedirs(datasets_dir) if not os.path.exists(datasets_dir) else ''\n",
    "model_dir=Path(\"./model/bert_checkpoints\")\n",
    "os.makedirs(model_dir) if not os.path.exists(model_dir) else ''\n",
    "print(\"Device:\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce219a97-9347-4d4a-9f69-1de8d58a9c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_train_data=pd.read_csv(datasets_dir.joinpath('train.csv'),encoding_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94c147d6-d864-4ee9-aa34-61a9c8d96c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_train_data['title']=pd_train_data['title'].fillna('')\n",
    "pd_train_data['abstract']=pd_train_data['abstract'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1041bb91-abff-439a-ab14-c31d264c58cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=pd.read_csv(datasets_dir.joinpath('test.csv'),encoding_errors='ignore')\n",
    "test_data['title']=test_data['title'].fillna('')\n",
    "test_data['abstract']=test_data['abstract'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "737990db-d5eb-407b-9ec4-e9facc28ec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_train_data['text'] = pd_train_data['title'].fillna('') + ' ' +  pd_train_data['author'].fillna('') + ' ' + pd_train_data['abstract'].fillna('')+ ' ' + pd_train_data['Keywords'].fillna('')\n",
    "test_data['text'] = test_data['title'].fillna('') + ' ' +  test_data['author'].fillna('') + ' ' + test_data['abstract'].fillna('')+ ' ' + pd_train_data['Keywords'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e53986e-7b15-4a08-8705-65d216b171dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>abstract</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Accessible Visual Artworks for Blind and Visua...</td>\n",
       "      <td>Quero, Luis Cavazos; Bartolome, Jorge Iranzo; ...</td>\n",
       "      <td>Despite the use of tactile graphics and audio ...</td>\n",
       "      <td>accessibility technology; multimodal interacti...</td>\n",
       "      <td>0</td>\n",
       "      <td>Accessible Visual Artworks for Blind and Visua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Seizure Detection and Prediction by Parallel M...</td>\n",
       "      <td>Li, Chenqi; Lammie, Corey; Dong, Xuening; Amir...</td>\n",
       "      <td>During the past two decades, epileptic seizure...</td>\n",
       "      <td>CNN; Seizure Detection; Seizure Prediction; EE...</td>\n",
       "      <td>1</td>\n",
       "      <td>Seizure Detection and Prediction by Parallel M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Fast ScanNet: Fast and Dense Analysis of Multi...</td>\n",
       "      <td>Lin, Huangjing; Chen, Hao; Graham, Simon; Dou,...</td>\n",
       "      <td>Lymph node metastasis is one of the most impor...</td>\n",
       "      <td>Histopathology image analysis; computational p...</td>\n",
       "      <td>1</td>\n",
       "      <td>Fast ScanNet: Fast and Dense Analysis of Multi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Long-Term Effectiveness of Antiretroviral Ther...</td>\n",
       "      <td>Huang, Peng; Tan, Jingguang; Ma, Wenzhe; Zheng...</td>\n",
       "      <td>In order to assess the effectiveness of the Ch...</td>\n",
       "      <td>HIV; ART; mortality; observational cohort stud...</td>\n",
       "      <td>0</td>\n",
       "      <td>Long-Term Effectiveness of Antiretroviral Ther...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Real-Time Facial Affective Computing on Mobile...</td>\n",
       "      <td>Guo, Yuanyuan; Xia, Yifan; Wang, Jing; Yu, Hui...</td>\n",
       "      <td>Convolutional Neural Networks (CNNs) have beco...</td>\n",
       "      <td>facial affective computing; convolutional neur...</td>\n",
       "      <td>0</td>\n",
       "      <td>Real-Time Facial Affective Computing on Mobile...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uuid                                              title  \\\n",
       "0     0  Accessible Visual Artworks for Blind and Visua...   \n",
       "1     1  Seizure Detection and Prediction by Parallel M...   \n",
       "2     2  Fast ScanNet: Fast and Dense Analysis of Multi...   \n",
       "3     3  Long-Term Effectiveness of Antiretroviral Ther...   \n",
       "4     4  Real-Time Facial Affective Computing on Mobile...   \n",
       "\n",
       "                                              author  \\\n",
       "0  Quero, Luis Cavazos; Bartolome, Jorge Iranzo; ...   \n",
       "1  Li, Chenqi; Lammie, Corey; Dong, Xuening; Amir...   \n",
       "2  Lin, Huangjing; Chen, Hao; Graham, Simon; Dou,...   \n",
       "3  Huang, Peng; Tan, Jingguang; Ma, Wenzhe; Zheng...   \n",
       "4  Guo, Yuanyuan; Xia, Yifan; Wang, Jing; Yu, Hui...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Despite the use of tactile graphics and audio ...   \n",
       "1  During the past two decades, epileptic seizure...   \n",
       "2  Lymph node metastasis is one of the most impor...   \n",
       "3  In order to assess the effectiveness of the Ch...   \n",
       "4  Convolutional Neural Networks (CNNs) have beco...   \n",
       "\n",
       "                                            Keywords  label  \\\n",
       "0  accessibility technology; multimodal interacti...      0   \n",
       "1  CNN; Seizure Detection; Seizure Prediction; EE...      1   \n",
       "2  Histopathology image analysis; computational p...      1   \n",
       "3  HIV; ART; mortality; observational cohort stud...      0   \n",
       "4  facial affective computing; convolutional neur...      0   \n",
       "\n",
       "                                                text  \n",
       "0  Accessible Visual Artworks for Blind and Visua...  \n",
       "1  Seizure Detection and Prediction by Parallel M...  \n",
       "2  Fast ScanNet: Fast and Dense Analysis of Multi...  \n",
       "3  Long-Term Effectiveness of Antiretroviral Ther...  \n",
       "4  Real-Time Facial Affective Computing on Mobile...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c2ad307-b04c-44dd-a7b4-481e85c51609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>abstract</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Monitoring Changes in Intracellular Reactive O...</td>\n",
       "      <td>Al-Hassan M Mustafa,Ramy Ashry,Oliver H Krämer...</td>\n",
       "      <td>Reactive oxygen species (ROS) are induced by s...</td>\n",
       "      <td>Flow cytometry; HDACi; Leukemia; ROS.</td>\n",
       "      <td>Monitoring Changes in Intracellular Reactive O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Source Printer Classification Using Printer Sp...</td>\n",
       "      <td>Joshi, Sharad; Khanna, Nitin</td>\n",
       "      <td>The knowledge of the source printer can help i...</td>\n",
       "      <td>Printer classification; local texture patterns...</td>\n",
       "      <td>Source Printer Classification Using Printer Sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Plasma-processed CoSn/RGO nanocomposite: A low...</td>\n",
       "      <td>Omelianovych, Oleksii; Larina, Liudmila L.; Oh...</td>\n",
       "      <td>The high cost of state-of-the-art Pt counter e...</td>\n",
       "      <td>Plasma reduction; Bimatalic alloy CoxSn1-x; Re...</td>\n",
       "      <td>Plasma-processed CoSn/RGO nanocomposite: A low...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Immediate Antiretroviral Therapy: The Need for...</td>\n",
       "      <td>Mgbako, Ofole; E. Sobieszczyk, Magdalena; Olen...</td>\n",
       "      <td>Immediate antiretroviral therapy (iART), defin...</td>\n",
       "      <td>HIV; antiretroviral therapy; rapid; health equity</td>\n",
       "      <td>Immediate Antiretroviral Therapy: The Need for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Design and analysis of an ultra-low-power LC q...</td>\n",
       "      <td>Lee, Kin Keung; Bryant, Carl; Tormanen, Markus...</td>\n",
       "      <td>This paper presents the design of an ultra-low...</td>\n",
       "      <td>Varactor; Spiral inductor; Quadrature generati...</td>\n",
       "      <td>Design and analysis of an ultra-low-power LC q...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uuid                                              title  \\\n",
       "0     0  Monitoring Changes in Intracellular Reactive O...   \n",
       "1     1  Source Printer Classification Using Printer Sp...   \n",
       "2     2  Plasma-processed CoSn/RGO nanocomposite: A low...   \n",
       "3     3  Immediate Antiretroviral Therapy: The Need for...   \n",
       "4     4  Design and analysis of an ultra-low-power LC q...   \n",
       "\n",
       "                                              author  \\\n",
       "0  Al-Hassan M Mustafa,Ramy Ashry,Oliver H Krämer...   \n",
       "1                       Joshi, Sharad; Khanna, Nitin   \n",
       "2  Omelianovych, Oleksii; Larina, Liudmila L.; Oh...   \n",
       "3  Mgbako, Ofole; E. Sobieszczyk, Magdalena; Olen...   \n",
       "4  Lee, Kin Keung; Bryant, Carl; Tormanen, Markus...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Reactive oxygen species (ROS) are induced by s...   \n",
       "1  The knowledge of the source printer can help i...   \n",
       "2  The high cost of state-of-the-art Pt counter e...   \n",
       "3  Immediate antiretroviral therapy (iART), defin...   \n",
       "4  This paper presents the design of an ultra-low...   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0              Flow cytometry; HDACi; Leukemia; ROS.   \n",
       "1  Printer classification; local texture patterns...   \n",
       "2  Plasma reduction; Bimatalic alloy CoxSn1-x; Re...   \n",
       "3  HIV; antiretroviral therapy; rapid; health equity   \n",
       "4  Varactor; Spiral inductor; Quadrature generati...   \n",
       "\n",
       "                                                text  \n",
       "0  Monitoring Changes in Intracellular Reactive O...  \n",
       "1  Source Printer Classification Using Printer Sp...  \n",
       "2  Plasma-processed CoSn/RGO nanocomposite: A low...  \n",
       "3  Immediate Antiretroviral Therapy: The Need for...  \n",
       "4  Design and analysis of an ultra-low-power LC q...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "617ee2d4-d11e-4fa7-9243-cff3b3fd7e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#从训练集中随机采样测试集\n",
    "#frac定义从训练集中采样的比例\n",
    "validation_data=pd_train_data.sample(frac=validation_ratio)\n",
    "#在train_data中取得index不在测试集的数据\n",
    "train_data=pd_train_data[~pd_train_data.index.isin(validation_data.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3e1be9e-645b-4c1f-bd9f-83fea3b20a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5400, 6000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data),len(pd_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e348ff23-bcbf-416b-95cd-fafeeb669e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建Dataset Dataset定义了数据集的内容，它相当于一个类似列表的数据结构，具有确定的长度，能够用索引获取数据集中的元素。\n",
    "#一般只需要实现getitem和len方法，一个是通过index返回features和labels，一个是获取数据集长度\n",
    "#与DataLoader交替进行来构建通往模型的数据管道\n",
    "class MyDataset(Dataset):\n",
    "    #从Dataset中继承\n",
    "    #定义构造函数\n",
    "    def __init__(self,mode='train'):\n",
    "        super(MyDataset,self).__init__()\n",
    "        self.mode=mode\n",
    "        if mode=='train':\n",
    "            self.dataset=train_data\n",
    "        elif mode=='validation':\n",
    "            self.dataset=validation_data\n",
    "        elif mode=='test':\n",
    "            self.dataset=test_data\n",
    "        else:\n",
    "            raise Exception(\"Unknown mode{}\".format(mode))\n",
    "    #定义从index取得数据\n",
    "    def __getitem__(self,index):\n",
    "        data=self.dataset.iloc[index]\n",
    "        text=data['text']\n",
    "        if self.mode=='test':\n",
    "            label=data['uuid']\n",
    "        else:\n",
    "            label=data['label']\n",
    "        return text,label\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "601d7e46-3071-4960-a9b8-71e33b07bf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=MyDataset('train')\n",
    "validation_dataset=MyDataset('validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1404ebb2-7eab-4330-b35e-2fe9f60cd03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Accessible Visual Artworks for Blind and Visually Impaired People: Comparing a Multimodal Approach with Tactile Graphics Quero, Luis Cavazos; Bartolome, Jorge Iranzo; Cho, Jundong Despite the use of tactile graphics and audio guides, blind and visually impaired people still face challenges to experience and understand visual artworks independently at art exhibitions. Art museums and other art places are increasingly exploring the use of interactive guides to make their collections more accessible. In this work, we describe our approach to an interactive multimodal guide prototype that uses audio and tactile modalities to improve the autonomous access to information and experience of visual artworks. The prototype is composed of a touch-sensitive 2.5D artwork relief model that can be freely explored by touch. Users can access localized verbal descriptions and audio by performing touch gestures on the surface while listening to themed background music along. We present the design requirements derived from a formative study realized with the help of eight blind and visually impaired participants, art museum and gallery staff, and artists. We extended the formative study by organizing two accessible art exhibitions. There, eighteen participants evaluated and compared multimodal and tactile graphic accessible exhibits. Results from a usability survey indicate that our multimodal approach is simple, easy to use, and improves confidence and independence when exploring visual artworks. accessibility technology; multimodal interaction; auditory interface; touch interface; vision impairment',\n",
       " 0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6af7f8a-0ab2-4ccd-a3e6-199dd815d3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A Scalable Near-Memory Architecture for Training Deep Neural Networks on Large In-Memory Datasets Schuiki, Fabian; Schaffner, Michael; Gurkaynak, Frank K.; Benini, Luca Most investigations into near-memory hardware accelerators for deep neural networks have primarily focused on inference, while the potential of accelerating training has received relatively little attention so far. Based on an in-depth analysis of the key computational patterns in state-of-the-art gradient-based training methods, we propose an efficient near-memory acceleration engine called NTX that can be used to train state-of-the-art deep convolutional neural networks at scale. Our main contributions are: (i) a loose coupling of RISC-V cores and NTX co-processors reducing offloading overhead by 7 x over previously published results; (ii) an optimized IEEE 754 compliant data path for fast high-precision convolutions and gradient propagation; (iii) evaluation of near-memory computing with NTX embedded into residual area on the Logic Base die of a Hybrid Memory Cube; and (iv) a scaling analysis to meshes of HMCs in a data center scenario. We demonstrate a 2.7 x energy efficiency improvement of NTX over contemporary GPUs at 4.4 x less silicon area, and a compute performance of 1.2 Tflop/s for training large state-of-the-art networks with full floating-point precision. At the data center scale, a mesh of NTX achieves above 95 percent parallel and energy efficiency, while providing 2.1 x energy savings or 3.1 x performance improvement over a GPU-based system. Parallel architectures; memory structures; memory hierarchy; machine learning; neural nets',\n",
       " 0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab8e5b9e-c50d-4858-8dd8-254ec624f355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer=AutoTokenizer.from_pretrained(\"/root/pc_code/exp4TCKE/my_model/bert-base-uncased\")\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"./my_model/bert-base-uncased/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52c60ab6-af88-4964-85ec-2b341204e18a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 2572, 1037, 2204, 2711, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"i am a good person\")\n",
    "#input_ids:存储着每个token在字典中的索引值\n",
    "#token_type_ids:literally识别句子的界限，单句分类任务则默认全为0，俩个句子输入的时候，第一个句子全为0，另一个全为1\n",
    "#attention_mask:界定注意力的范围，1表示被关注的位置，纳入下游任务计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c49f81c3-27c1-41ee-9081-f4869974b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#构造Dataloader中的collate_fn实现对句子编码、填充、组装batch\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    将一个batch的文本句子转成tensor，并组成batch。\n",
    "    :param batch: 一个batch的句子，例如: [('推文', target), ('推文', target), ...]\n",
    "    :return: 处理后的结果，例如：\n",
    "             src: {'input_ids': tensor([[ 101, ..., 102, 0, 0, ...], ...]), 'attention_mask': tensor([[1, ..., 1, 0, ...], ...])}\n",
    "             target：[1, 1, 0, ...]\n",
    "    \"\"\"\n",
    "    text,label=zip(*batch)\n",
    "    text,label=list(text),list(label)\n",
    "    # src is to be sent to bert, so no special processing is required, just use the tokenizer result directly\n",
    "    # padding='max_length' padding for insufficient length\n",
    "    #parameter return_tensors='pt', returns the pytorch tensor type\n",
    "    # truncation=True .cut if the length is too long\n",
    "    src=tokenizer(text,padding='max_length',max_length=text_max_length,return_tensors='pt',truncation=True)\n",
    "    return src,torch.LongTensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a27df5e-c6f2-49de-85fa-d34b4b0af42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=DataLoader(train_dataset,batch_size=batch_size,shuffle=True,collate_fn=collate_fn)\n",
    "validation_loader=DataLoader(validation_dataset,batch_size=batch_size,shuffle=False,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "670426a8-660c-48f8-832c-cb5442fe64ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: {'input_ids': tensor([[  101,  4895,  6342,  ..., 12515,  1996,   102],\n",
      "        [  101,  9349, 29477,  ...,  4958, 11921,   102],\n",
      "        [  101,  2051,  2186,  ...,  1011,  1996,   102],\n",
      "        ...,\n",
      "        [  101,  3145,  5876,  ...,  1996, 18749,   102],\n",
      "        [  101,  7170,  1011,  ...,  2836,  1997,   102],\n",
      "        [  101, 12702, 21102,  ..., 11628,  3824,   102]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "targets: tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "inputs,targets=next(iter(train_loader))#next() 返回迭代器的下一个项目。next() 函数要和生成迭代器的 iter() 函数一起使用。\n",
    "print(\"inputs:\",inputs)\n",
    "print(\"targets:\",targets)\n",
    "#input_ids: stores the index value of each token in the dictionary\n",
    "#token_type_ids: Literally identify the boundaries of sentences, and the single-sentence classification task defaults to all 0. When two sentences are input, the first sentence is all 0, and the other is all 1\n",
    "#attention_mask: Define the scope of attention, 1 indicates the position of attention, included in the calculation of downstream tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b31cd98f-fbaf-428d-bb41-f3ed859caba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the pre-training model, which consists of the bert basic model and the prediction layer\n",
    "class MyModel(nn.Module): \n",
    "\n",
    "    def __init__(self):\n",
    "        super(MyModel,self).__init__()\n",
    "        #load bert model from local disk\n",
    "        self.bert=BertModel.from_pretrained(\"./my_model/bert-base-uncased/\")\n",
    "        #the prediction layer \n",
    "        self.predictor=nn.Sequential(\n",
    "            #The feature vector output by bert is generally 786 dimensions\n",
    "            nn.Linear(768,256), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(256,1),\n",
    "            nn.Sigmoid() \n",
    "        )\n",
    "    def forward(self,src): \n",
    "        '''\n",
    "        :param src: Text data after tokenizer\n",
    "        \n",
    "        Unpack the src direct sequence and pass it to bert, because bert and tokenizer are a set, so you can do this.\n",
    "         Get the output of the encoder, and use the output of the front [CLS] as the input of the final linear layer\n",
    "         **src Pass src as a dictionary of keywords\n",
    "         Take the shape of last_hidden_state as (batch_size, max_length, hidden_size)=(16,128,786)\n",
    "         [:, 0, :] slices the output, retaining the hidden state corresponding to the first token of the sample.\n",
    "         Since the input to BERT is a series of tokens, the first token is usually a special \"[CLS]\" token, \n",
    "         which is calculated to contain the semantic information of the entire sentence.\n",
    "         Extract the hidden state of the first token ([CLS] token) corresponding to each sample\n",
    "        '''\n",
    "        outputs=self.bert(**src).last_hidden_state[:,0,:]\n",
    "        return self.predictor(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a40eaec3-d323-411b-8151-a18f64b88ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MyModel() \n",
    "model=model.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4693d171-7821-4e73-87fe-0d88adad9d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用二元交叉熵和Adam优化器\n",
    "criteria=nn.BCELoss()\n",
    "opimizer=torch.optim.Adam(model.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8bddcba-2e1a-4eeb-8c30-4f9a205a19fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#由于inputs是字典类型，定义一个函数可以支持gpu运算\n",
    "def to_device(dic_tensors): \n",
    "    result_tensor={}\n",
    "    for key,value in dic_tensors.items():\n",
    "        result_tensor[key]=value.to(device)\n",
    "    return result_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5304a1b7-4912-4493-8e18-2f7ca5f128ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m429750130\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77a694f4-6fec-4576-9b45-ce3200e715db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/adminroot/code/pc/exp4TCKE/wandb/run-20230728_215458-aqs9igea</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/429750130/bert4text_categorization/runs/aqs9igea' target=\"_blank\">light-sunset-7</a></strong> to <a href='https://wandb.ai/429750130/bert4text_categorization' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/429750130/bert4text_categorization' target=\"_blank\">https://wandb.ai/429750130/bert4text_categorization</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/429750130/bert4text_categorization/runs/aqs9igea' target=\"_blank\">https://wandb.ai/429750130/bert4text_categorization/runs/aqs9igea</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/429750130/bert4text_categorization/runs/aqs9igea?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f858b8450f0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"bert4text_categorization\",\n",
    "    config={\n",
    "        \"learning_rate\":lr,\n",
    "        \"epochs\":epochs, \n",
    "        \"batch_size\":batch_size,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d87e0ea0-c4af-440c-a21e-5bd087fab669",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义验证方法，获取accuracy和loss\n",
    "def validate():\n",
    "    model.eval()\n",
    "    total_loss=0.\n",
    "    total_corrate=0\n",
    "    for inputs,targets in validation_loader:\n",
    "        inputs,targets=to_device(inputs),targets.to(device)\n",
    "        outputs=model(inputs)\n",
    "        loss=criteria(outputs.view(-1),targets.float())\n",
    "        total_loss+=float(loss)\n",
    "        correct_num=(((outputs>=0.5).float()*1).flatten()==targets).sum()\n",
    "        total_corrate+=correct_num\n",
    "    acc=total_corrate/len(validation_dataset)\n",
    "    loss=total_loss/len(validation_dataset)\n",
    "    # wandb.log({\"acc\":acc,\"loss\":loss})\n",
    "    return acc,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "106fc76c-79fe-4c87-96e5-f925804434ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "# memory=5800/240 #(GPU total memory/number of cores)\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]=\"max_split_size_mb:{}\".format(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e216119a-d9b3-463b-920b-0231a7108cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▆█▆▇██▆█▆▄▇▇▇▇████▄▄▃▃▃▃▅▇▆▄▅▃▃▅▅▅▅▅▅▅▅▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>total loss</td><td>▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▂</td></tr><tr><td>validation loss</td><td>▁▁▃▃▁▃▄▅▅▄▅▅▆▆▆▇▇▇▄▆▇▇██▇▄▃▆▇▅▃▅▅▅▆▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.945</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>step</td><td>33800</td></tr><tr><td>total loss</td><td>0.46664</td></tr><tr><td>validation loss</td><td>0.02322</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">light-sunset-7</strong> at: <a href='https://wandb.ai/429750130/bert4text_categorization/runs/aqs9igea' target=\"_blank\">https://wandb.ai/429750130/bert4text_categorization/runs/aqs9igea</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230728_215458-aqs9igea/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#training mode\n",
    "model.train()\n",
    "#刷新cuda缓存\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "total_loss=0.\n",
    "step=0 \n",
    "best_accuracy=0\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for i,(inputs,targets) in enumerate(train_loader):\n",
    "        inputs,targets=to_device(inputs),targets.to(device)\n",
    "        outputs=model(inputs)   \n",
    "        loss=criteria(outputs.view(-1),targets.float())\n",
    "        loss.backward()\n",
    "        opimizer.step()\n",
    "        opimizer.zero_grad()\n",
    "        total_loss+=float(loss)\n",
    "        step+=1\n",
    "        if step%log_per_step==0:\n",
    "            wandb.log({\n",
    "                \"epoch\":epoch,\n",
    "                \"step\":step,\n",
    "                \"total loss\":total_loss\n",
    "            })\n",
    "            total_loss=0\n",
    "        del inputs,targets\n",
    "    accuracy,validation_loss=validate()\n",
    "    wandb.log({\n",
    "        \"epoch\":epoch,\n",
    "        \"accuracy\":accuracy,\n",
    "        \"validation loss\":validation_loss\n",
    "    })\n",
    "    torch.save(model,model_dir/f\"model_{epoch}.pt\")\n",
    "    if accuracy>best_accuracy:\n",
    "        torch.save(model,model_dir/f\"model_best.pt\")\n",
    "        best_accuracy=accuracy\n",
    "# 保存代码\n",
    "arti_code=wandb.Artifact('ipynb',type='code')\n",
    "arti_code.add_file('./bert4text_categorization.ipynb')\n",
    "wandb.log_artifact(arti_code)\n",
    "arti_model=wandb.Artifact('bert',type='model')\n",
    "arti_model.add_file('./model/bert_checkpoints/model_best.pt')\n",
    "wandb.log_artifact(arti_model)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b2efe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load(model_dir/f\"model_best.pt\")\n",
    "model=model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "840dcb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset=MyDataset('test')\n",
    "test_loader=DataLoader(test_dataset,batch_size=batch_size,shuffle=False,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0710215f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "for inputs,ids in test_loader:\n",
    "    outputs=model(to_device(inputs))\n",
    "    outputs=(outputs>=0.5).int().flatten().tolist()\n",
    "    ids=ids.tolist()\n",
    "    results=results+[(id,result) for result,id in zip(outputs,ids)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "397b030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lable=[pair[1] for pair in results]\n",
    "test_data['label']=test_lable\n",
    "test_data[['uuid','Keywords','label']].to_csv('submit_task3.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb6cb7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
